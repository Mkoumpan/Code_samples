{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#                      **Problem Statement:**\n",
        "\n",
        "  # **Multimodal Mental Health Monitoring with LLM Explanations**\n",
        "\n",
        "**Objective:** Develop a multimodal AI system that predicts mental health states (e.g., stress, anxiety, depression) in college students by integrating high-frequency physiological time-series data from wearables with contextual features (e.g., activity, location). The system should not only make accurate predictions but also provide human-readable natural language explanations of the physiological and contextual patterns driving each prediction using a Large Language Model (LLM).\n",
        "\n",
        "**Challenges:**\n",
        "\n",
        "  * Temporal Dynamics: Modeling per-second wearable sensor data to capture short-term and long-term physiological trends.\n",
        "\n",
        "  * Multimodal Fusion: Combining numeric time-series embeddings with categorical contextual information in a form suitable for reasoning by an LLM.\n",
        "\n",
        "  * Natural Language Reasoning: Translating sensor signals and context into interpretable textual explanations for mental health states.\n",
        "\n",
        "  * Noise & Missing Data: Handling artifacts, missing seconds, and irregular sensor readings without degrading prediction or explanation quality.\n",
        "\n",
        "  * Evaluation of Explanations: Measuring both prediction accuracy and the relevance/clarity of the LLM-generated reasoning.\n",
        "\n",
        "\n",
        "**Solution Approach:**\n",
        "\n",
        "* Aggregate per-second sensor readings to manageable windows (e.g., per minute) and encode as temporal embeddings.\n",
        "\n",
        "* Convert categorical context/activity features into textual tokens for the LLM (e.g., “Student is in the library”) or embeddings.\n",
        "\n",
        "* Fuse temporal embeddings and textual embeddings via cross-attention layers so the LLM can jointly reason over physiology and context.\n",
        "\n",
        "* Train the model to output both:\n",
        "\n",
        "* Predicted mental health state per window, and\n",
        "\n",
        "* A natural language explanation summarizing which physiological trends and contextual factors contributed to the prediction.\n",
        "\n",
        "* Evaluate using classification metrics (accuracy, F1-score) for predictions and human/automatic metrics (BLEU, ROUGE, clinical interpretability) for explanations."
      ],
      "metadata": {
        "id": "Ua-86gjJDG9j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYuhYfkdCDjR",
        "outputId": "c13650ba-c21a-434b-ac3b-8c11fa42ca53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ziya07/mental-health-monitoring-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 124k/124k [00:00<00:00, 45.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/ziya07/mental-health-monitoring-dataset/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ziya07/mental-health-monitoring-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to dataset folder\n",
        "dataset_dir = path  # e.g., '/root/.cache/kagglehub/datasets/programmer3/wearable-sensor-data-for-mental-health-prediction/versions/3'\n",
        "\n",
        "# List files in the directory\n",
        "print()\n",
        "\n",
        "# Suppose the main CSV is \"sensor_data.csv\"\n",
        "csv_file = os.path.join(dataset_dir, os.listdir(dataset_dir)[0])\n",
        "\n",
        "# Load into DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Check\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdPGySKyCJo6",
        "outputId": "5181de91-f6df-4c67-e515-51545218b796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "             timestamp student_id  heart_rate  skin_temperature       eda  \\\n",
            "0  2024-07-31 09:00:00        S01   84.967142         33.088032  0.513965   \n",
            "1  2024-07-31 09:00:01        S01   78.617357         33.140445  0.506502   \n",
            "2  2024-07-31 09:00:02        S01   86.476885         34.524906  0.581328   \n",
            "3  2024-07-31 09:00:03        S01   95.230299         33.500341  0.415615   \n",
            "4  2024-07-31 09:00:04        S01   77.658466         33.891065  0.325633   \n",
            "\n",
            "   physical_activity stress_level  stress_label context_activity  \\\n",
            "0                  1          NaN             0             Dorm   \n",
            "1                  2          NaN             0             Dorm   \n",
            "2                  2          NaN             0        Cafeteria   \n",
            "3                  1          NaN             0              Gym   \n",
            "4                  1          NaN             0          Library   \n",
            "\n",
            "   session_duration  \n",
            "0                69  \n",
            "1                52  \n",
            "2                69  \n",
            "3               103  \n",
            "4               103  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_time_windows(data, window_size=60, step_size=1, target_col=None):\n",
        "    \"\"\"\n",
        "    Converts a time-series array into rolling windows for temporal modeling.\n",
        "\n",
        "    Parameters:\n",
        "    - data: np.array of shape (num_timesteps, num_features)\n",
        "    - window_size: int, number of time steps per input window\n",
        "    - step_size: int, how many steps to move the window each time\n",
        "    - target_col: int or None, column index for target values; if None, returns all features\n",
        "\n",
        "    Returns:\n",
        "    - X: np.array of shape (num_windows, window_size, num_features)\n",
        "    - y: np.array of shape (num_windows,) if target_col provided, else None\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for start in range(0, len(data) - window_size, step_size):\n",
        "        end = start + window_size\n",
        "        X.append(data[start:end, :])\n",
        "        if target_col is not None:\n",
        "            y.append(data[end, target_col])  # predict next time step\n",
        "    X = np.array(X)\n",
        "    y = np.array(y) if target_col is not None else None\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "nRwq4_IJCbPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5-_M5dIHMZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-CoHZnRHMgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solutionn with cross attention/window data\n"
      ],
      "metadata": {
        "id": "REjg9CtHHQRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suggested solution"
      ],
      "metadata": {
        "id": "xp6eoINLXj7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multimodal_mental_health_full_tf.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Rolling window conversion\n",
        "# -------------------------------\n",
        "def create_rolling_windows(df, time_features, context_feature, target, window_size=5, agg_interval=60, step_size=1):\n",
        "    X_sensor, X_context, y = [], [], []\n",
        "\n",
        "    for student in df['Student'].unique():\n",
        "        student_df = df[df['Student'] == student].reset_index(drop=True)\n",
        "\n",
        "        # Aggregate per interval\n",
        "        num_intervals = len(student_df) // agg_interval\n",
        "        agg_features, agg_context, agg_labels = [], [], []\n",
        "        for i in range(num_intervals):\n",
        "            window_df = student_df.iloc[i*agg_interval:(i+1)*agg_interval]\n",
        "            mean_features = window_df[time_features].mean().values\n",
        "            std_features = window_df[time_features].std().values\n",
        "            agg_features.append(np.concatenate([mean_features, std_features]))\n",
        "            agg_context.append(window_df[context_feature].mode()[0])\n",
        "            agg_labels.append(window_df[target].mode()[0])\n",
        "\n",
        "        agg_features = np.array(agg_features)\n",
        "        agg_labels = np.array(agg_labels)\n",
        "\n",
        "        # Create rolling sequences\n",
        "        for start in range(0, len(agg_features) - window_size + 1, step_size):\n",
        "            end = start + window_size\n",
        "            X_sensor.append(agg_features[start:end])\n",
        "            X_context.append(agg_context[start:end])\n",
        "            y.append(agg_labels[end-1])\n",
        "\n",
        "    X_sensor = np.array(X_sensor, dtype=np.float32)\n",
        "    y = np.array(y)\n",
        "    return X_sensor, X_context, y\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Time-series encoder (LSTM)\n",
        "# -------------------------------\n",
        "def build_time_series_encoder(input_dim, hidden_dim=64):\n",
        "    inputs = layers.Input(shape=(None, input_dim))  # sequence length, features\n",
        "    x = layers.LSTM(hidden_dim, return_sequences=True)(inputs)\n",
        "    return Model(inputs, x, name=\"TimeSeriesEncoder\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Cross-attention layer\n",
        "# -------------------------------\n",
        "class CrossAttentionLayer(layers.Layer):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.query_dense = layers.Dense(hidden_dim)\n",
        "        self.key_dense = layers.Dense(hidden_dim)\n",
        "        self.value_dense = layers.Dense(hidden_dim)\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "\n",
        "    def call(self, query, key, value, mask=None):\n",
        "        Q = self.query_dense(query)\n",
        "        K = self.key_dense(key)\n",
        "        V = self.value_dense(value)\n",
        "\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / self.scale\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
        "        output = tf.matmul(attn_weights, V)\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Multimodal model with cross-attention\n",
        "# -------------------------------\n",
        "def build_multimodal_model(sensor_input_dim, num_classes=3, llm_model_name=\"distilbert-base-uncased\", hidden_dim=64):\n",
        "    # Time-series encoder\n",
        "    ts_encoder = build_time_series_encoder(sensor_input_dim, hidden_dim)\n",
        "\n",
        "    # LLM encoder\n",
        "    llm_encoder = TFAutoModel.from_pretrained(llm_model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "\n",
        "    # Inputs\n",
        "    sensor_input = layers.Input(shape=(None, sensor_input_dim), name='sensor')  # rolling window sequence\n",
        "    context_input_ids = layers.Input(shape=(None,), dtype=tf.int32, name='context_input_ids')\n",
        "    context_attention_mask = layers.Input(shape=(None,), dtype=tf.int32, name='context_attention_mask')\n",
        "\n",
        "    # Encodings\n",
        "    sensor_emb_seq = ts_encoder(sensor_input)  # (batch, seq_len, hidden_dim)\n",
        "    context_emb_seq = llm_encoder(input_ids=context_input_ids, attention_mask=context_attention_mask).last_hidden_state\n",
        "\n",
        "    # Cross-attention: context attends to sensor sequence\n",
        "    cross_attention = CrossAttentionLayer(hidden_dim)\n",
        "    fused_emb = cross_attention(query=context_emb_seq, key=sensor_emb_seq, value=sensor_emb_seq)\n",
        "\n",
        "    # Pool over context tokens\n",
        "    fused_emb = tf.reduce_mean(fused_emb, axis=1)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.Dense(128, activation='relu')(fused_emb)\n",
        "    preds = layers.Dense(num_classes, activation='softmax', name='prediction')(x)\n",
        "\n",
        "    model = Model(inputs=[sensor_input, context_input_ids, context_attention_mask], outputs=preds)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Main script: dataset, training\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your per-second dataset\n",
        "    #df = pd.read_csv(\"mental_health_sensor_data.csv\")\n",
        "\n",
        "    time_features = ['Acceleration X', 'Acceleration Y', 'Acceleration Z',\n",
        "                     'Gyroscope X', 'Gyroscope Y', 'Gyroscope Z',\n",
        "                     'Temperature', 'Humidity', 'Light', 'Exposure']\n",
        "    context_feature = 'Activity'\n",
        "    target = 'Health State'\n",
        "\n",
        "    # Create rolling windows (past 5 minutes per sequence)\n",
        "    X_sensor, X_context, y = create_rolling_windows(df, time_features, context_feature, target,\n",
        "                                                    window_size=5, agg_interval=60, step_size=1)\n",
        "\n",
        "    # Tokenize context sequences (flatten sequences for simplicity)\n",
        "    llm_model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "    context_texts = [\" \".join(seq) for seq in X_context]\n",
        "    encodings = tokenizer(context_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "    # Build model\n",
        "    sensor_input_dim = len(time_features) * 2  # mean + std\n",
        "    model, tokenizer = build_multimodal_model(sensor_input_dim, num_classes=3, llm_model_name=llm_model_name)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train (example, 1 epoch)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'sensor': X_sensor,\n",
        "            'context_input_ids': encodings['input_ids'],\n",
        "            'context_attention_mask': encodings['attention_mask']\n",
        "        },\n",
        "        y\n",
        "    )).shuffle(100).batch(8)\n",
        "\n",
        "    model.fit(dataset, epochs=1)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 6. Evaluation after training\n",
        "    # -------------------------------\n",
        "    X_eval_sensor = X_sensor\n",
        "    context_input_ids_eval = encodings['input_ids']\n",
        "    context_attention_mask_eval = encodings['attention_mask']\n",
        "\n",
        "    y_true = y\n",
        "    y_pred_probs = model.predict({\n",
        "        'sensor': X_eval_sensor,\n",
        "        'context_input_ids': context_input_ids_eval,\n",
        "        'context_attention_mask': context_attention_mask_eval\n",
        "    })\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7. Example predictions\n",
        "    # -------------------------------\n",
        "    example_idx = 0\n",
        "    print(\"\\n--- Example Input ---\")\n",
        "    print(\"Sensor features (rolling windows):\\n\", X_eval_sensor[example_idx])\n",
        "    print(\"\\nContext text sequence:\\n\", X_context[example_idx])\n",
        "    print(\"\\nTrue label:\", y_true[example_idx])\n",
        "    print(\"Predicted label:\", y_pred[example_idx])\n",
        "\n",
        "    top3_idx = np.argsort(y_pred_probs[example_idx])[::-1][:3]\n",
        "    print(\"Top 3 predicted probabilities:\", [(i, y_pred_probs[example_idx][i]) for i in top3_idx])\n"
      ],
      "metadata": {
        "id": "_LkrwaClKE5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}